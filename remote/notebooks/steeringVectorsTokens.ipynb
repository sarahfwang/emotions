{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0ca8179",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0f0d209",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformer_lens==2.11.0\n",
      "  Downloading transformer_lens-2.11.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting einops\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting eindex-callum\n",
      "  Downloading eindex_callum-0.1.2-py3-none-any.whl.metadata (377 bytes)\n",
      "Collecting jaxtyping\n",
      "  Downloading jaxtyping-0.3.4-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-1.2.3-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting accelerate>=0.23.0 (from transformer_lens==2.11.0)\n",
      "  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting beartype<0.15.0,>=0.14.1 (from transformer_lens==2.11.0)\n",
      "  Downloading beartype-0.14.1-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting better-abc<0.0.4,>=0.0.3 (from transformer_lens==2.11.0)\n",
      "  Downloading better_abc-0.0.3-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting datasets>=2.7.1 (from transformer_lens==2.11.0)\n",
      "  Downloading datasets-4.4.2-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting fancy-einsum>=0.0.3 (from transformer_lens==2.11.0)\n",
      "  Downloading fancy_einsum-0.0.3-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from transformer_lens==2.11.0) (1.26.3)\n",
      "Collecting pandas>=1.1.5 (from transformer_lens==2.11.0)\n",
      "  Downloading pandas-2.3.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
      "Collecting rich>=12.6.0 (from transformer_lens==2.11.0)\n",
      "  Downloading rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting sentencepiece (from transformer_lens==2.11.0)\n",
      "  Downloading sentencepiece-0.2.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.11/dist-packages (from transformer_lens==2.11.0) (2.4.1+cu124)\n",
      "Collecting tqdm>=4.64.1 (from transformer_lens==2.11.0)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting transformers>=4.37.2 (from transformer_lens==2.11.0)\n",
      "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting typeguard<5.0,>=4.2 (from transformer_lens==2.11.0)\n",
      "  Downloading typeguard-4.4.4-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from transformer_lens==2.11.0) (4.9.0)\n",
      "Collecting wandb>=0.13.5 (from transformer_lens==2.11.0)\n",
      "  Downloading wandb-0.23.1-py3-none-manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
      "Collecting wadler-lindig>=0.1.3 (from jaxtyping)\n",
      "  Downloading wadler_lindig-0.1.7-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2024.2.0)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface_hub)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (0.27.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
      "Collecting shellingham (from huggingface_hub)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting typer-slim (from huggingface_hub)\n",
      "  Downloading typer_slim-0.21.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.23.0->transformer_lens==2.11.0) (6.0.0)\n",
      "Collecting safetensors>=0.4.3 (from accelerate>=0.23.0->transformer_lens==2.11.0)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting pyarrow>=21.0.0 (from datasets>=2.7.1->transformer_lens==2.11.0)\n",
      "  Downloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets>=2.7.1->transformer_lens==2.11.0)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.7.1->transformer_lens==2.11.0) (2.32.3)\n",
      "Collecting xxhash (from datasets>=2.7.1->transformer_lens==2.11.0)\n",
      "  Downloading xxhash-3.6.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.19 (from datasets>=2.7.1->transformer_lens==2.11.0)\n",
      "  Downloading multiprocess-0.70.18-py311-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface_hub) (4.6.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface_hub) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface_hub) (1.0.5)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface_hub) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub) (0.14.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->transformer_lens==2.11.0) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas>=1.1.5->transformer_lens==2.11.0)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=1.1.5->transformer_lens==2.11.0)\n",
      "  Downloading tzdata-2025.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=12.6.0->transformer_lens==2.11.0)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.6.0->transformer_lens==2.11.0) (2.18.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->transformer_lens==2.11.0) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->transformer_lens==2.11.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->transformer_lens==2.11.0) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->transformer_lens==2.11.0) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->transformer_lens==2.11.0) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->transformer_lens==2.11.0) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->transformer_lens==2.11.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.2.65 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->transformer_lens==2.11.0) (12.4.2.65)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.0.44 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->transformer_lens==2.11.0) (11.2.0.44)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.119 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->transformer_lens==2.11.0) (10.3.5.119)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.0.99 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->transformer_lens==2.11.0) (11.6.0.99)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.0.142 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->transformer_lens==2.11.0) (12.3.0.142)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->transformer_lens==2.11.0) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->transformer_lens==2.11.0) (12.4.99)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->transformer_lens==2.11.0) (12.4.99)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->transformer_lens==2.11.0) (3.0.0)\n",
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.37.2->transformer_lens==2.11.0)\n",
      "  Downloading regex-2025.11.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers>=4.37.2->transformer_lens==2.11.0)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens==2.11.0) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens==2.11.0) (2.2.3)\n",
      "Collecting typing-extensions (from transformer_lens==2.11.0)\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting click>=8.0.1 (from wandb>=0.13.5->transformer_lens==2.11.0)\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb>=0.13.5->transformer_lens==2.11.0)\n",
      "  Downloading gitpython-3.1.46-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer_lens==2.11.0) (4.3.6)\n",
      "Collecting protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 (from wandb>=0.13.5->transformer_lens==2.11.0)\n",
      "  Downloading protobuf-6.33.2-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting pydantic<3 (from wandb>=0.13.5->transformer_lens==2.11.0)\n",
      "  Downloading pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb>=0.13.5->transformer_lens==2.11.0)\n",
      "  Downloading sentry_sdk-2.48.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.7.1->transformer_lens==2.11.0)\n",
      "  Downloading aiohttp-3.13.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens==2.11.0)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens==2.11.0)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3->wandb>=0.13.5->transformer_lens==2.11.0)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic<3->wandb>=0.13.5->transformer_lens==2.11.0)\n",
      "  Downloading pydantic_core-2.41.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic<3->wandb>=0.13.5->transformer_lens==2.11.0)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->transformer_lens==2.11.0) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10->transformer_lens==2.11.0) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.10->transformer_lens==2.11.0) (1.3.0)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.7.1->transformer_lens==2.11.0)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.7.1->transformer_lens==2.11.0)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.7.1->transformer_lens==2.11.0) (24.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.7.1->transformer_lens==2.11.0)\n",
      "  Downloading frozenlist-1.8.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.7.1->transformer_lens==2.11.0)\n",
      "  Downloading multidict-6.7.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.7.1->transformer_lens==2.11.0)\n",
      "  Downloading propcache-0.4.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.7.1->transformer_lens==2.11.0)\n",
      "  Downloading yarl-1.22.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens==2.11.0)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Downloading transformer_lens-2.11.0-py3-none-any.whl (177 kB)\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Downloading eindex_callum-0.1.2-py3-none-any.whl (8.3 kB)\n",
      "Downloading jaxtyping-0.3.4-py3-none-any.whl (56 kB)\n",
      "Downloading accelerate-1.12.0-py3-none-any.whl (380 kB)\n",
      "Downloading beartype-0.14.1-py3-none-any.whl (739 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.7/739.7 kB\u001b[0m \u001b[31m223.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading better_abc-0.0.3-py3-none-any.whl (3.5 kB)\n",
      "Downloading datasets-4.4.2-py3-none-any.whl (512 kB)\n",
      "Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n",
      "Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m112.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.3.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m123.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m306.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m110.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typeguard-4.4.4-py3-none-any.whl (34 kB)\n",
      "Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Downloading wadler_lindig-0.1.7-py3-none-any.whl (20 kB)\n",
      "Downloading wandb-0.23.1-py3-none-manylinux_2_28_x86_64.whl (22.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m144.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Downloading gitpython-3.1.46-py3-none-any.whl (208 kB)\n",
      "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading multiprocess-0.70.18-py311-none-any.whl (144 kB)\n",
      "Downloading protobuf-6.33.2-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
      "Downloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (47.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m97.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Downloading pydantic_core-2.41.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m208.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading regex-2025.11.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (800 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.4/800.4 kB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
      "Downloading sentry_sdk-2.48.0-py2.py3-none-any.whl (414 kB)\n",
      "Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m127.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.3-py2.py3-none-any.whl (348 kB)\n",
      "Downloading xxhash-3.6.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "Downloading aiohttp-3.13.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.8.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (231 kB)\n",
      "Downloading multidict-6.7.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\n",
      "Downloading propcache-0.4.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (210 kB)\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Downloading yarl-1.22.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (365 kB)\n",
      "Installing collected packages: pytz, better-abc, xxhash, wadler-lindig, tzdata, typing-extensions, tqdm, smmap, sentry-sdk, sentencepiece, safetensors, regex, pyarrow, protobuf, propcache, multidict, mdurl, hf-xet, frozenlist, fancy-einsum, einops, dill, click, beartype, annotated-types, aiohappyeyeballs, yarl, typing-inspection, typeguard, pydantic-core, pandas, multiprocess, markdown-it-py, jaxtyping, huggingface_hub, gitdb, aiosignal, tokenizers, rich, pydantic, gitpython, aiohttp, wandb, transformers, eindex-callum, accelerate, datasets, transformer_lens\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.9.0\n",
      "    Uninstalling typing_extensions-4.9.0:\n",
      "      Successfully uninstalled typing_extensions-4.9.0\n",
      "Successfully installed accelerate-1.12.0 aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 annotated-types-0.7.0 beartype-0.14.1 better-abc-0.0.3 click-8.3.1 datasets-4.4.2 dill-0.4.0 eindex-callum-0.1.2 einops-0.8.1 fancy-einsum-0.0.3 frozenlist-1.8.0 gitdb-4.0.12 gitpython-3.1.46 hf-xet-1.2.0 huggingface_hub-0.36.0 jaxtyping-0.3.4 markdown-it-py-4.0.0 mdurl-0.1.2 multidict-6.7.0 multiprocess-0.70.18 pandas-2.3.3 propcache-0.4.1 protobuf-6.33.2 pyarrow-22.0.0 pydantic-2.12.5 pydantic-core-2.41.5 pytz-2025.2 regex-2025.11.3 rich-14.2.0 safetensors-0.7.0 sentencepiece-0.2.1 sentry-sdk-2.48.0 smmap-5.0.2 tokenizers-0.22.1 tqdm-4.67.1 transformer_lens-2.11.0 transformers-4.57.3 typeguard-4.4.4 typing-extensions-4.15.0 typing-inspection-0.4.2 tzdata-2025.3 wadler-lindig-0.1.7 wandb-0.23.1 xxhash-3.6.0 yarl-1.22.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformer_lens==2.11.0 einops eindex-callum jaxtyping huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76b0f2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "import gc\n",
    "import numpy as np\n",
    "import re\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from eindex import eindex\n",
    "from IPython.display import display\n",
    "from jaxtyping import Float, Int\n",
    "from torch import Tensor\n",
    "from tqdm import tqdm\n",
    "from transformer_lens import (\n",
    "    ActivationCache,\n",
    "    FactoredMatrix,\n",
    "    HookedTransformer,\n",
    "    HookedTransformerConfig,\n",
    "    utils,\n",
    ")\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "device = t.device(\n",
    "    \"mps\" if t.backends.mps.is_available() else \"cuda\" if t.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14dd3ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe7719d6b23640d5b6141b0d76ce4a33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bcf20be186b45c2a4741416e92ae3ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "285b3b378f7c48b1a27d23e33b38cd48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa72c76b2bf44a128589bedafaf0d51b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4632bbb9cde4e05b6edea1c06ffe706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7291aa7efe1d4bc2b35539449ddce3fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a22ab78458943f4ac6e59e4810351fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f402f986ec4c431b9c673e24f4c515b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81eadafe62124f2dbeb2235e2d50a2c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f844a6487e5d4b33ae886339fad13170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2d176b45c664f4db4faa753db38888a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "988711b1d6e04bf5bebec71ab820d852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Llama-3.1-8B-Instruct into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "hf_token = \"\" # TODO: fill in\n",
    "login(token=hf_token)\n",
    "model = HookedTransformer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "293b8d49-0ecb-47df-b9a0-081101b3d37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_mem():\n",
    "    # Memory currently allocated by tensors\n",
    "    print(f\"Allocated: {t.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    \n",
    "    # Memory reserved by the caching allocator (includes unused cached memory)\n",
    "    print(f\"Reserved: {t.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "\n",
    "def clear_mem():\n",
    "    gc.collect()\n",
    "    t.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698805b5",
   "metadata": {},
   "source": [
    "# Get prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edcb38c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict, List\n",
    "\n",
    "def load_prompt_batches(prompts_file: str, N: int = None) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Load prompts and return as a dictionary for easier access.\n",
    "    \n",
    "    Args:\n",
    "        prompts_file: Path to the JSON file containing prompts\n",
    "        N: Number of prompt sets to process (if None, processes all)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with keys:\n",
    "        - '1st_person_negative'\n",
    "        - '1st_person_positive'\n",
    "        - '3rd_person_negative'\n",
    "        - '3rd_person_positive'\n",
    "        Each key maps to a list of prompt strings\n",
    "    \"\"\"\n",
    "    # Load prompts from file\n",
    "    with open(prompts_file, 'r') as f:\n",
    "        prompts_data = json.load(f)\n",
    "    \n",
    "    # Filter to only process first N prompt sets if specified\n",
    "    if N is not None:\n",
    "        prompts_data = [p for p in prompts_data if p['id'] <= N]\n",
    "    \n",
    "    # Initialize dictionary\n",
    "    batches = {\n",
    "        '1st_person_negative': [],\n",
    "        '1st_person_positive': [],\n",
    "        '3rd_person_negative': [],\n",
    "        '3rd_person_positive': []\n",
    "    }\n",
    "    \n",
    "    # Populate the batches\n",
    "    for prompt_set in prompts_data:\n",
    "        for prompt_type in batches.keys():\n",
    "            batches[prompt_type].append(prompt_set['prompts'][prompt_type])\n",
    "    \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "597b5e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current wd is notebooks/\n",
    "prompt_batches = load_prompt_batches(\"../data/prompts.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0137dc",
   "metadata": {},
   "source": [
    "## Last token activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f06ad34",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_activations = {}\n",
    "\n",
    "def names_filter(name):\n",
    "    res = (\"resid_pre\" in name) or (\"resid_mid\" in name) \\\n",
    "        or name == utils.get_act_name(\"resid_post\", model.cfg.n_layers-1)\n",
    "    return res\n",
    "\n",
    "def post_resid_hook(\n",
    "    resid: Float[Tensor, \"batch seq_len d_model\"],\n",
    "    hook: HookPoint\n",
    "):\n",
    "    \"\"\"\n",
    "    Writes running mean of last-token activations to all_activations.\n",
    "    \"\"\"\n",
    "    global prompt_count\n",
    "\n",
    "    current_activation = resid[:, -1, :].cpu()\n",
    "\n",
    "    if hook.name not in all_activations:\n",
    "        all_activations[hook.name] = current_activation\n",
    "    else:\n",
    "        all_activations[hook.name] = all_activations[hook.name] + \\\n",
    "            (current_activation - all_activations[hook.name]) / (prompt_count + 1)\n",
    "\n",
    "def write_activations(activations, file_name):\n",
    "    \"\"\"\n",
    "    Save activations dictionary to a file.\n",
    "    \n",
    "    Args:\n",
    "        activations: Dict[str, Tensor] - Dictionary with layer names as keys and tensors as values\n",
    "        file_name: str - Path to save file (typically .pt or .pth extension)\n",
    "    \"\"\"\n",
    "    # Save the entire dictionary\n",
    "    t.save(activations, file_name)\n",
    "    print(f\"Saved activations to {file_name}\")\n",
    "\n",
    "def read_activations(file_name):\n",
    "    \"\"\"Load activations from file.\"\"\"\n",
    "    activations = t.load(file_name, weights_only=False)\n",
    "    return activations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd88ae5-60f7-4b44-a96b-8137d4b9b15c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for batch_name, batch in prompt_batches.items():\n",
    "    # Clear activations\n",
    "    all_activations.clear()\n",
    "    prompt_count = 0\n",
    "\n",
    "    # Accumulate avg. activation\n",
    "    print(f\"Processing {batch_name}\", all_activations, prompt_count)\n",
    "    for prompt in batch:\n",
    "        with t.no_grad():\n",
    "            _ = model.run_with_hooks(\n",
    "                prompt,\n",
    "                fwd_hooks=[(names_filter, post_resid_hook)],\n",
    "                return_type=None  # Don't return logits\n",
    "            )\n",
    "        \n",
    "        clear_mem()\n",
    "        prompt_count += 1\n",
    "\n",
    "    # Write results\n",
    "    write_activations(all_activations, f\"{batch_name}.pt\")\n",
    "    \n",
    "    model.reset_hooks()  # Clean up hooks\n",
    "    clear_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d1e207",
   "metadata": {},
   "source": [
    "## Steering vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74712652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_steering_vectors(neg_dataset_path, pos_dataset_path) -> Dict[str, Float[Tensor, \"d_model\"]]:\n",
    "    \"\"\"\n",
    "    Get steering vectors between two datasets.\n",
    "    Difference in terms of neg_dataset - pos_dataset.\n",
    "    \"\"\"\n",
    "    activations_neg = t.load(neg_dataset_path)\n",
    "    activations_pos = t.load(pos_dataset_path)\n",
    "\n",
    "    steering_vectors = {\n",
    "      name: (activations_neg[name] - activations_pos[name])\n",
    "      for name in activations_neg.keys()\n",
    "    }\n",
    "    return steering_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5663f842",
   "metadata": {},
   "source": [
    "## Steering Vector Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1db2167f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "# Baseline\n",
    "def baseline_word(\n",
    "  model: HookedTransformer, \n",
    "  prompt: str,\n",
    "  max_new_tokens: int = 6\n",
    ") -> str:\n",
    "  tokens = model.to_tokens(prompt, prepend_bos=True)\n",
    "  with t.no_grad():\n",
    "    logits = model.generate(\n",
    "      tokens,\n",
    "      max_new_tokens=max_new_tokens,\n",
    "      do_sample=False\n",
    "    )\n",
    "    logits = logits[:, -max_new_tokens:].squeeze(0)\n",
    "    output_str = regularize_output(model.to_string(logits))\n",
    "    return output_str\n",
    "\n",
    "def baseline_results(\n",
    "  model: HookedTransformer,\n",
    "  neg_prompts: List[str],\n",
    "  pos_prompts: List[str],\n",
    ") -> Dict[str, Dict[str, int]]:\n",
    "  res = {}\n",
    "\n",
    "  neg_prompt_word_freq = defaultdict(int)\n",
    "  pos_prompt_word_freq = defaultdict(int)\n",
    "  for prompt in neg_prompts:\n",
    "    word = baseline_word(model, prompt)\n",
    "    neg_prompt_word_freq[word] += 1\n",
    "  \n",
    "  for prompt in pos_prompts:\n",
    "    word = baseline_word(model, prompt)\n",
    "    pos_prompt_word_freq[word] += 1\n",
    "  \n",
    "  res[\"neg_prompt\"] = neg_prompt_word_freq\n",
    "  res[\"pos_prompt\"] = pos_prompt_word_freq\n",
    "  \n",
    "  return res\n",
    "\n",
    "\n",
    "# Add steering vector\n",
    "def hook_add_steering_vector(\n",
    "  resid: Float[Tensor, \"batch seq_len d_model\"],\n",
    "  hook: HookPoint,\n",
    "  alpha: float,\n",
    "  steering_vector: Float[Tensor, \"d_model\"]\n",
    "):\n",
    "  resid += alpha * steering_vector\n",
    "\n",
    "def add_steering_vector(\n",
    "  model: HookedTransformer,\n",
    "  prompt: str,\n",
    "  steering_vector: Float[Tensor, \"d_model\"],\n",
    "  steering_vector_name: str,\n",
    "  alpha = 1.0,\n",
    "  max_new_tokens: int = 6\n",
    ") -> List[int]:\n",
    "  \"\"\"\n",
    "  Generate text with steering vector intervention.\n",
    "  \"\"\"\n",
    "  tmp_hook = functools.partial(hook_add_steering_vector, alpha=alpha, steering_vector=steering_vector)\n",
    "\n",
    "  # Tokenize the prompt\n",
    "  tokens = model.to_tokens(prompt, prepend_bos=True)\n",
    "  generated_tokens = []\n",
    "\n",
    "  # Generate max_new_tokens tokens\n",
    "  for _ in range(max_new_tokens):\n",
    "    with t.no_grad():\n",
    "      logits = model.run_with_hooks(\n",
    "        tokens,\n",
    "        fwd_hooks=[(steering_vector_name, tmp_hook)]\n",
    "      )\n",
    "      model.reset_hooks()\n",
    "\n",
    "    final_logits = logits[:, -1, :]\n",
    "    next_token = final_logits.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "    generated_tokens.append(next_token.item())\n",
    "    tokens = t.cat([tokens, next_token], dim=1)\n",
    "\n",
    "  return generated_tokens\n",
    "\n",
    "# Ablate steering vector\n",
    "def hook_ablate_steering_vector(\n",
    "  resid: Float[Tensor, \"batch seq_len d_model\"],\n",
    "  hook: HookPoint,\n",
    "  steering_vector: Float[Tensor, \"1 d_model\"]\n",
    "):\n",
    "  unit_steering_vector = (steering_vector / t.norm(steering_vector)).squeeze(0) # [d_model,]\n",
    "  coeff = resid @ unit_steering_vector        # [batch, seq_len]\n",
    "  resid -= coeff.unsqueeze(-1) * unit_steering_vector\n",
    "\n",
    "def ablate_steering_vector(\n",
    "  model: HookedTransformer,\n",
    "  prompt: str,\n",
    "  steering_vector: Float[Tensor, \"d_model\"],\n",
    "  max_new_tokens: int = 6\n",
    ") -> List[int]:\n",
    "  tmp_hook = functools.partial(hook_ablate_steering_vector, steering_vector=steering_vector)\n",
    "  tokens = model.to_tokens(prompt, prepend_bos=True)\n",
    "  generated_tokens = []\n",
    "\n",
    "  for _ in range(max_new_tokens):\n",
    "    with t.no_grad():\n",
    "      logits = model.run_with_hooks(\n",
    "        tokens,\n",
    "        fwd_hooks=[(names_filter, tmp_hook)]\n",
    "      )\n",
    "      model.reset_hooks()\n",
    "\n",
    "    final_logits = logits[:, -1, :]\n",
    "    next_token = final_logits.argmax(dim=-1, keepdim=True)\n",
    "    tokens = t.cat([tokens, next_token], dim=1)\n",
    "    generated_tokens.append(next_token.item())\n",
    "\n",
    "  return generated_tokens\n",
    "\n",
    "# KL divergence w/ and w/out ablation\n",
    "def kl_divergence(p: Tensor, q: Tensor) -> Tensor:\n",
    "    \"\"\"KL(p || q)\"\"\"\n",
    "    log_p = F.log_softmax(p, dim=-1)\n",
    "    log_q = F.log_softmax(q, dim=-1)\n",
    "    return F.kl_div(log_q, log_p, reduction='sum', log_target=True)\n",
    "\n",
    "def kl_divergence_of_last_token(\n",
    "  model: HookedTransformer,\n",
    "  prompt: str,\n",
    "  steering_vector: Float[Tensor, \"d_model\"],\n",
    "  baseline_logits: Float[Tensor, \"d_vocab\"],\n",
    ") -> float:\n",
    "  tmp_hook = functools.partial(hook_ablate_steering_vector, steering_vector=steering_vector)\n",
    "  tokens = model.to_tokens(prompt, prepend_bos=True)\n",
    "\n",
    "  # Get the last token logits\n",
    "  with t.no_grad():\n",
    "    logits_ablation = model.run_with_hooks(\n",
    "      tokens,\n",
    "      fwd_hooks=[(names_filter, tmp_hook)]\n",
    "    )\n",
    "    model.reset_hooks()\n",
    "    last_token_logits_ablation = logits_ablation[:, -1, :].squeeze(0).squeeze(0) # [d_vocab,]\n",
    "    del logits_ablation\n",
    "  \n",
    "  # Clear memory\n",
    "  clear_mem()\n",
    "\n",
    "  # Compute the KL divergence\n",
    "  kl_score = kl_divergence(baseline_logits, last_token_logits_ablation)\n",
    "\n",
    "  return kl_score\n",
    "\n",
    "def get_baseline_logits(\n",
    "  model: HookedTransformer,\n",
    "  prompt: str\n",
    ") -> Float[Tensor, \"d_vocab\"]:\n",
    "  tokens = model.to_tokens(prompt, prepend_bos=True)\n",
    "  with t.no_grad():\n",
    "    logits = model.run_with_hooks(tokens, fwd_hooks=[])\n",
    "    model.reset_hooks()\n",
    "    return logits[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afdbac84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "def regularize_output(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Regularize model output to a single word:\n",
    "    - Strip whitespace\n",
    "    - Remove periods\n",
    "    - Convert to lowercase\n",
    "    - Extract first word if multiple words\n",
    "    \"\"\"\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'[.,!?;:]', '', text)\n",
    "    text = text.split()[0] if text.split() else text\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def steering_vector_results(\n",
    "  model: HookedTransformer,\n",
    "  neg_prompts: List[str],\n",
    "  pos_prompts: List[str],\n",
    "  steering_vectors: Dict[str, Float[Tensor, \"d_model\"]]\n",
    "):\n",
    "  res = {}\n",
    "\n",
    "  start_whole = time.perf_counter()\n",
    "  \n",
    "  # Get baseline logits\n",
    "  baseline_logits_dict = {}\n",
    "  for prompt in pos_prompts:\n",
    "    baseline_logits_dict[prompt] = get_baseline_logits(model, prompt).to(\"cpu\")\n",
    "\n",
    "  for layer_name, steering_vector in steering_vectors.items():\n",
    "    print(f\"Processing steering vector {layer_name}\")\n",
    "    start = time.perf_counter()\n",
    "    # Move steering vector to device\n",
    "    sv = steering_vector.to(model.cfg.device)\n",
    "    ablate_neg_from_neg_word_freq = defaultdict(int)\n",
    "    add_neg_to_pos_word_freq = defaultdict(int)\n",
    "    ablate_neg_from_pos_avg_kl_score = 0\n",
    "\n",
    "    # Ablation\n",
    "    for prompt in neg_prompts:\n",
    "      tokens_ablated = ablate_steering_vector(\n",
    "        model,\n",
    "        prompt,\n",
    "        sv,\n",
    "      )\n",
    "      word_ablated = regularize_output(model.to_string(tokens_ablated))\n",
    "      ablate_neg_from_neg_word_freq[word_ablated] += 1\n",
    "    print(f\"Time elapsed for ablation: {time.perf_counter() - start:.3f} s\")\n",
    "\n",
    "    # Addition\n",
    "    start_add = time.perf_counter()\n",
    "    for prompt in pos_prompts:\n",
    "      tokens_added = add_steering_vector(\n",
    "        model,\n",
    "        prompt,\n",
    "        sv,\n",
    "        layer_name,\n",
    "      )\n",
    "      word_added = regularize_output(model.to_string(tokens_added))\n",
    "      add_neg_to_pos_word_freq[word_added] += 1\n",
    "    print(f\"Time elapsed for addition: {time.perf_counter() - start_add:.3f} s\")\n",
    "\n",
    "    # KL\n",
    "    start_kl = time.perf_counter()\n",
    "    for prompt in pos_prompts:\n",
    "      baseline_logits = baseline_logits_dict[prompt].to(model.cfg.device)\n",
    "      kl_score = kl_divergence_of_last_token(\n",
    "        model,\n",
    "        prompt,\n",
    "        sv,\n",
    "        baseline_logits,\n",
    "      )\n",
    "      ablate_neg_from_pos_avg_kl_score += kl_score.item()\n",
    "      # print(\"kl score\", kl_score.item())\n",
    "    ablate_neg_from_pos_avg_kl_score /= len(pos_prompts) # Take average\n",
    "    print(f\"Time elapsed for kl: {time.perf_counter() - start_kl:.3f} s\")\n",
    "\n",
    "    res[layer_name] = {\n",
    "      \"ablate_neg_from_neg\": ablate_neg_from_neg_word_freq,\n",
    "      \"add_neg_to_pos\": add_neg_to_pos_word_freq,\n",
    "      \"ablate_neg_from_pos_avg_kl_score\": ablate_neg_from_pos_avg_kl_score\n",
    "    }\n",
    "  print(f\"Total time elapsed: {time.perf_counter() - start_whole:.3f} s\")\n",
    "  return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb12502-76be-4e42-8b84-5932089ededa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1114/2025794274.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  activations_neg = t.load(neg_dataset_path)\n",
      "/tmp/ipykernel_1114/2025794274.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  activations_pos = t.load(pos_dataset_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing steering vector blocks.0.hook_resid_pre\n"
     ]
    }
   ],
   "source": [
    "# Test ablation\n",
    "neg_prompts = prompt_batches[\"1st_person_negative\"]\n",
    "pos_prompts = prompt_batches[\"1st_person_positive\"]\n",
    "steering_vectors = get_steering_vectors(\"out/1st_person_negative.pt\", \"out/1st_person_positive.pt\")\n",
    "\n",
    "res = steering_vector_results(\n",
    "  model,\n",
    "  neg_prompts,\n",
    "  pos_prompts,\n",
    "  steering_vectors\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9efd7d",
   "metadata": {},
   "source": [
    "## Writing helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00462a86-3405-46d0-ba5a-b782ff0b214f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_steering_results(res, file_name):\n",
    "    with open(file_name, \"w\") as f:\n",
    "        json.dump(res, f, indent=2)\n",
    "def read_steering_results(file_name):\n",
    "    with open(file_name, \"r\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c79c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_steering_results(res, \"10_prompt_steering_results_ablation_only.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd0415b",
   "metadata": {},
   "source": [
    "## Write baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de908d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_res = baseline_results(model, neg_prompts, pos_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015acd69",
   "metadata": {},
   "source": [
    "# Clean steering vector results\n",
    "(removing extra quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8297d043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sarahwang/Documents/Projects/emotion\n",
      "Cleaned remote/notebooks/3rd_person_50_prompts_real_all_results.json -> remote/notebooks/3rd_person_50_prompts_real_all_results_cleaned.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "def clean_json_quotes(input_file, output_file):\n",
    "    \"\"\"Remove extra quotes from JSON keys/values like 'word' -> word\"\"\"\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    def clean_dict(d):\n",
    "        if isinstance(d, dict):\n",
    "            return {re.sub(r\"^'|'$\", \"\", k): clean_dict(v) for k, v in d.items()}\n",
    "        elif isinstance(d, list):\n",
    "            return [clean_dict(item) for item in d]\n",
    "        else:\n",
    "            return d\n",
    "    \n",
    "    cleaned = clean_dict(data)\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(cleaned, f, indent=2)\n",
    "    \n",
    "    print(f\"Cleaned {input_file} -> {output_file}\")\n",
    "\n",
    "# Usage\n",
    "clean_json_quotes(\"remote/notebooks/3rd_person_50_prompts_real_all_results.json\", \n",
    "                  \"remote/notebooks/3rd_person_50_prompts_real_all_results_cleaned.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec030bd",
   "metadata": {},
   "source": [
    "# Experiment with best steering vectors on diff prompts, scratchpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e123df5-40cb-444c-8a03-8ca4dcfa4a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_139/2025794274.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  activations_neg = t.load(neg_dataset_path)\n",
      "/tmp/ipykernel_139/2025794274.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  activations_pos = t.load(pos_dataset_path)\n"
     ]
    }
   ],
   "source": [
    "steering_vectors = get_steering_vectors(\"out/activations/1st_person_negative.pt\", \"out/activations/1st_person_positive.pt\")\n",
    "\n",
    "# ['blocks.20.hook_resid_mid', 'blocks.21.hook_resid_pre', 'blocks.21.hook_resid_mid', 'blocks.19.hook_resid_pre']\n",
    "\n",
    "sv_name = 'blocks.19.hook_resid_pre'\n",
    "sv = steering_vectors[sv_name].to(model.cfg.device)\n",
    "sv = sv.squeeze(0)\n",
    "\n",
    "prompt = \"Write a diary entry that a person might have:\"\n",
    "\n",
    "tokens = add_steering_vector(\n",
    "    model,\n",
    "    prompt,\n",
    "    sv,\n",
    "    sv_name,\n",
    "    alpha = 5.0,\n",
    "    max_new_tokens = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32eda8c4-f470-4682-841c-5550278356ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Negative. Emotion: Embarrassment. Intensity: 6/10. The text describes\n",
      " Negative. Emotion: Embarrassment. Intensity: 6/10. The text describes\n",
      " Negative. Emotion: Embarrassment. Intensity: 6/10. The text describes\n",
      " Negative. Emotion: Embarrassment. Intensity: 6/10. The text describes\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Analyze the emotional content of the following text. Text: 'You join a group discussion and others continue talking as if you are not there'. Sentiment (Positive/Negative):\"\"\"\n",
    "\n",
    "for sv_name in ['blocks.22.hook_resid_pre', 'blocks.21.hook_resid_mid', 'blocks.21.hook_resid_pre', 'blocks.20.hook_resid_mid', 'blocks.19.hook_resid_pre']:\n",
    "    sv = steering_vectors[sv_name].to(model.cfg.device)\n",
    "    sv = sv.squeeze(0)\n",
    "\n",
    "    tokens = ablate_steering_vector(\n",
    "        model,\n",
    "        prompt,\n",
    "        sv,\n",
    "        max_new_tokens = 1\n",
    "    )\n",
    "    \n",
    "    # tokens = add_steering_vector(\n",
    "    #     model,\n",
    "    #     prompt,\n",
    "    #     sv,\n",
    "    #     sv_name,\n",
    "    #     alpha = 1.0,\n",
    "    #     max_new_tokens = 1\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7794aaa9-47da-4ec7-947b-1948ad7944d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "steering_vectors = get_steering_vectors(\"out/activations/1st_person_negative.pt\", \"out/activations/1st_person_positive.pt\")\n",
    "\n",
    "sv_name = 'blocks.19.hook_resid_pre'\n",
    "sv = steering_vectors[sv_name].to(model.cfg.device)\n",
    "sv = sv.squeeze(0)\n",
    "\n",
    "prompt = \"Write a diary entry that a person might have:\"\n",
    "\n",
    "tokens_res = {}\n",
    "for layer in range(1, model.cfg.n_layers+1):\n",
    "  layer_name = f\"blocks.{layer}.hook_resid_pre\"\n",
    "  tokens = add_steering_vector(\n",
    "      model,\n",
    "      prompt,\n",
    "      sv,\n",
    "      layer_name,\n",
    "      alpha = 3.0,\n",
    "      max_new_tokens = 100\n",
    "  )\n",
    "  tokens_res[layer_name] = tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
